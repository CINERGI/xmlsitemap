{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Aggregate statistics on CI_onlineResource elements from CINERGI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "import sys\n",
    "from lxml import etree  #supposed to be better than xml.etree\n",
    "from urllib.parse import urlsplit, urlunsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#geoportalBaseURL = 'http://cinergi.sdsc.edu/geoportal/'\n",
    "geoportalBaseURL = 'http://datadiscoverystudio.org/geoportal/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getlinks(documentID, metadataURLx, filehandle, elt):\n",
    "    \n",
    "    linkcount = 0\n",
    "\n",
    " #iterate through CI_OnlineResource elements and write to filehandle\n",
    "    for onlineres in elt.getiterator(\"{http://www.isotc211.org/2005/gmd}CI_OnlineResource\"):\n",
    "        try:\n",
    "            if (onlineres.find(\"gmd:linkage/gmd:URL\",namespaces=NSMAP) is not None):\n",
    "                theURL=onlineres.find(\"gmd:linkage/gmd:URL\",namespaces=NSMAP).text\n",
    "                if (theURL is None): continue\n",
    "            else:\n",
    "                continue #don't bother if there's no URL!\n",
    "\n",
    "            if (onlineres.find(\"gmd:name/gco:CharacterString\",namespaces=NSMAP) is not None):\n",
    "                thename=onlineres.find(\"gmd:name/gco:CharacterString\",namespaces=NSMAP).text\n",
    "                if (thename is None): thename=''\n",
    "            else:\n",
    "                thename=''\n",
    "\n",
    "            if (onlineres.find(\"gmd:description/gco:CharacterString\",namespaces=NSMAP) is not None):\n",
    "                thedescription=onlineres.find(\"gmd:description/gco:CharacterString\",namespaces=NSMAP).text\n",
    "                if (thedescription is None): thedescription=''\n",
    "            else:\n",
    "                thedescription=''\n",
    "\n",
    "            if (onlineres.find(\"gmd:protocol/gco:CharacterString\",namespaces=NSMAP) is not None):\n",
    "                theprotocol=onlineres.find(\"gmd:protocol/gco:CharacterString\",namespaces=NSMAP).text\n",
    "                if (theprotocol is None): theprotocol=''\n",
    "            else:\n",
    "                theprotocol=''\n",
    "\n",
    "            if (onlineres.find(\"gmd:applicationProfile/gco:CharacterString\",namespaces=NSMAP) is not None):\n",
    "                theappprofile=onlineres.find(\"gmd:applicationProfile/gco:CharacterString\",namespaces=NSMAP).text\n",
    "                if (theappprofile is None): theappprofile=''\n",
    "            else:\n",
    "                theappprofile=''\n",
    "\n",
    "            if (onlineres.find(\"gmd:function/gmd:CI_OnLineFunctionCode\",namespaces=NSMAP) is not None):\n",
    "                if (onlineres.find(\"gmd:function/gmd:CI_OnLineFunctionCode\",namespaces=NSMAP).get(\"codeListValue\") is not None):\n",
    "                    thefunctioncode=onlineres.find(\"gmd:function/gmd:CI_OnLineFunctionCode\",namespaces=NSMAP).get(\"codeListValue\")\n",
    "                else:\n",
    "                    thefunctioncode=''\n",
    "            else:\n",
    "                thefunctioncode=''\n",
    "\n",
    "            if (onlineres.find(\"gmd:function/gmd:CI_OnLineFunctionCode\",namespaces=NSMAP) is not None):    \n",
    "                thefunctiontext=onlineres.find(\"gmd:function/gmd:CI_OnLineFunctionCode\",namespaces=NSMAP).text\n",
    "                if (thefunctiontext is None): thefunctiontext=''\n",
    "            else:\n",
    "                thefunctiontext=''\n",
    "\n",
    "            #print('\\n Distribution: name-%s;\\n  url- %s; \\n  description--%s; \\n   protocol-%s, app profile- %s; function- %s; %s' %\n",
    "            #      (thename,theURL,thedescription,theprotocol,theappprofile,thefunctioncode,thefunctiontext))\n",
    "\n",
    "            #dist_list.append({\"name\":thename,\"url\":theURL,\"description\":thedescription,\"protocol\":theprotocol,\n",
    "            #                  \"app_profile\":theappprofile,\"functioncode\":thefunctioncode,\"functiontext\":thefunctiontext})\n",
    "\n",
    "            split_url = urlsplit(theURL)\n",
    "            baseurl= split_url.scheme + \"://\" + split_url.netloc\n",
    "            thepath= split_url.path \n",
    "            thequery= split_url.query   \n",
    "            thefragment=split_url.fragment\n",
    "\n",
    "            filehandle.write(documentID + ',' +\n",
    "                             thename + ',' +\n",
    "                             theURL + ',' +\n",
    "                             baseurl + ',' +\n",
    "                             thepath + ',' +\n",
    "                             thequery + ',' +\n",
    "                             thefragment + ',' +\n",
    "                             thedescription + ',' +\n",
    "                             theprotocol + ',' +\n",
    "                             theappprofile + ',' +\n",
    "                             thefunctioncode + ',' +\n",
    "                             thefunctiontext)\n",
    "            filehandle.write('\\n')\n",
    "            linkcount = linkcount + 1\n",
    "            # print('linkcount: %s' % linkcount)\n",
    "        except:\n",
    "            print('error processing CI_OnlineResource, document: %s \\n' % documentID)\n",
    "            continue\n",
    "            \n",
    "    return  linkcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop\n",
    "\n",
    "Writes out a set of csv files with {name, url, baseurl, path, query, fragment, description, protocol, appprofile, functioncode, functiontext} for each ci_onlineResource element found in the distribution or SV_Operations sections of the metadata record.\n",
    "the parameter determines the number of records written to each csv. From my home, takes about 270 ms per record. Would take about 80 hours to run through all CINERGI catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\tmp\\\n",
      "request1:  http://datadiscoverystudio.org/geoportal/elastic/metadata/item/_search?scroll=200m&size=10000&_source=link_s\n",
      "total records:  1669369\n",
      "scrollID: DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAilzFnJUSXNpcmxmU1NTRENuTFFVTzRObFEAAAAAAAKZrhZfTndQME5WQ1FPdS13MWp6WUItUWlRAAAAAAACKXQWclRJc2lybGZTU1NEQ25MUVVPNE5sUQAAAAAAApmwFl9Od1AwTlZDUU91LXcxanpZQi1RaVEAAAAAAAKZrxZfTndQME5WQ1FPdS13MWp6WUItUWlR\n",
      "skipping; counter is 10000\n",
      "scrollID: DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAilzFnJUSXNpcmxmU1NTRENuTFFVTzRObFEAAAAAAAKZrhZfTndQME5WQ1FPdS13MWp6WUItUWlRAAAAAAACKXQWclRJc2lybGZTU1NEQ25MUVVPNE5sUQAAAAAAApmwFl9Od1AwTlZDUU91LXcxanpZQi1RaVEAAAAAAAKZrxZfTndQME5WQ1FPdS13MWp6WUItUWlR\n",
      "skipping; counter is 20000\n",
      "scrollID: DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAilzFnJUSXNpcmxmU1NTRENuTFFVTzRObFEAAAAAAAKZrhZfTndQME5WQ1FPdS13MWp6WUItUWlRAAAAAAACKXQWclRJc2lybGZTU1NEQ25MUVVPNE5sUQAAAAAAApmwFl9Od1AwTlZDUU91LXcxanpZQi1RaVEAAAAAAAKZrxZfTndQME5WQ1FPdS13MWp6WUItUWlR\n",
      "skipping; counter is 30000\n",
      "scrollID: DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAilzFnJUSXNpcmxmU1NTRENuTFFVTzRObFEAAAAAAAKZrhZfTndQME5WQ1FPdS13MWp6WUItUWlRAAAAAAACKXQWclRJc2lybGZTU1NEQ25MUVVPNE5sUQAAAAAAApmwFl9Od1AwTlZDUU91LXcxanpZQi1RaVEAAAAAAAKZrxZfTndQME5WQ1FPdS13MWp6WUItUWlR\n",
      "problem parsing http://datadiscoverystudio.org/geoportal/rest/metadata/item/9314cd4508c241c99bf00758ed86dcfb/xml\n",
      "counter = 39999, totallinkcount = 18741, filecount = 3\n",
      "scrollID: DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAilzFnJUSXNpcmxmU1NTRENuTFFVTzRObFEAAAAAAAKZrhZfTndQME5WQ1FPdS13MWp6WUItUWlRAAAAAAACKXQWclRJc2lybGZTU1NEQ25MUVVPNE5sUQAAAAAAApmwFl9Od1AwTlZDUU91LXcxanpZQi1RaVEAAAAAAAKZrxZfTndQME5WQ1FPdS13MWp6WUItUWlR\n",
      "problem parsing http://datadiscoverystudio.org/geoportal/rest/metadata/item/e4978ba57c1c40fba446c380293cc233/xml\n",
      "counter = 49998, totallinkcount = 39164, filecount = 4\n",
      "scrollID: DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAilzFnJUSXNpcmxmU1NTRENuTFFVTzRObFEAAAAAAAKZrhZfTndQME5WQ1FPdS13MWp6WUItUWlRAAAAAAACKXQWclRJc2lybGZTU1NEQ25MUVVPNE5sUQAAAAAAApmwFl9Od1AwTlZDUU91LXcxanpZQi1RaVEAAAAAAAKZrxZfTndQME5WQ1FPdS13MWp6WUItUWlR\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#set up namespace map for ISO metadata\n",
    "NSMAP = {\"gmi\":\"http://www.isotc211.org/2005/gmi\" ,\n",
    "    \"gco\":\"http://www.isotc211.org/2005/gco\" ,\n",
    "    \"gmd\":\"http://www.isotc211.org/2005/gmd\" ,\n",
    "    \"gml\":\"http://www.opengis.net/gml\" ,\n",
    "    \"gmx\":\"http://www.isotc211.org/2005/gmx\" ,\n",
    "    \"gts\":\"http://www.isotc211.org/2005/gts\" ,\n",
    "    \"srv\":\"http://www.isotc211.org/2005/srv\" ,\n",
    "    \"xlink\":\"http://www.w3.org/1999/xlink\"}\n",
    "\n",
    "fileLocationBase = 'c:\\\\tmp\\\\'\n",
    "print (fileLocationBase)\n",
    "\n",
    "# construct Elasticsearch URL with  search request\n",
    "espath= geoportalBaseURL + \"elastic/\"\n",
    "esindex=\"metadata\"\n",
    "esresource=\"/item/_search\"\n",
    "baseURL = espath+esindex+esresource\n",
    "catalogURL = geoportalBaseURL\n",
    "response = ''\n",
    "\n",
    "# need to use scrolling because there are >10000 records\n",
    "# this is the time to live for the scroll index; renewed on each search call\n",
    "p_scroll=\"200m\"\n",
    "#number of records to write in each csv file\n",
    "p_size=\"10000\"\n",
    "#p_size=\"10\"\n",
    "\n",
    "# comma delimited list of index fields to return from the _source section of the hits object\n",
    "#p_source=\"sys_modified_dt,title\"\n",
    "p_source=\"link_s\"\n",
    "\n",
    "# first get the scroll index to start scrolling loop, and the total number of records\n",
    "\n",
    "counter = 0\n",
    "totallinkcount = 0\n",
    "filecount=3\n",
    "#first request to get scrolling set up\n",
    "p = {'scroll':p_scroll, 'size' : p_size, '_source' : p_source}\n",
    "r = requests.get(baseURL, params=p)\n",
    "print (\"request1: \", r.url)\n",
    "\n",
    "if (r.status_code == requests.codes.ok):\n",
    "        respon = r.json()\n",
    "        totalRecords = respon[\"hits\"][\"total\"]\n",
    "        scrollID = respon[\"_scroll_id\"]\n",
    "        print (\"total records: \", totalRecords)\n",
    "else:\n",
    "        r.raise_for_status()\n",
    "        sys.exit(0)\n",
    "\n",
    "skip = filecount * int(p_size) #how many records to skip before writing output. \n",
    "recordlimit = totalRecords   # set to lower value for testing\n",
    "\n",
    "while counter < totalRecords:\n",
    "#while counter <= recordlimit:\n",
    "        #have to hit the scroll resource for Elasticsearch        \n",
    "        esresource=\"_search/scroll\"\n",
    "        #cinergi requires publisher role to run the scroll resource\n",
    "        espath=\"http://admin:admin@datadiscoverystudio.org/geoportal/elastic/\"\n",
    "        baseURL = espath+esresource\n",
    "        p = { 'scroll':p_scroll, \n",
    "        'scroll_id' : scrollID}\n",
    "        r = requests.get(baseURL, params=p)\n",
    "        #print (\"request: \", r.url)\n",
    "    #        print \"raw response2: \", r, \" status: \", r.status_code\n",
    "    #        print r.headers['content-type']\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            respon = r.json()\n",
    "            #print(response)  #the response has the record id's to scan for links\n",
    "            scrollID = respon[\"_scroll_id\"]\n",
    "            print ('scrollID: %s' % scrollID)\n",
    "        else:\n",
    "            print ('response status: %s' % r.status_code)\n",
    "            print ('\\n')\n",
    "            print(\"counter: %s, url: %s \\n\" % (counter, baseURL))\n",
    "            print(\"scroll: %s, scroll_id: %s \\n\" % (p_scroll, scrollID))\n",
    "#            r.raise_for_status()\n",
    "#            break\n",
    "            continue\n",
    "       \n",
    "  # don't log records that we want to skip      \n",
    "        if counter < skip: \n",
    "            counter = counter + int(p_size)\n",
    "            print ('skipping; counter is %s' % counter)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            file_object  = open(fileLocationBase + \"CinergiDistribution\" +str(filecount) + \".csv\", \"w\", encoding='utf-8')\n",
    "        except:\n",
    "            print(\"ERROR: Can't open the output file, bailing out\")\n",
    "            print(sys.exc_info()[1])\n",
    "            sys.exit(0)\n",
    "\n",
    "        filehandle = file_object\n",
    "        filehandle.write('documentID,name,url,baseurl,path,query,fragment, description,protocol,appprofile,functioncode,functiontext')\n",
    "        filehandle.write('\\n')  \n",
    "        \n",
    "    #dist_list=[]\n",
    "    #process the json for each record\n",
    "    # create pandas frame with distribution/ or operations/CI_OnlineResource element\n",
    "    \n",
    "        for record in respon[\"hits\"][\"hits\"]:\n",
    "            documentID = record[\"_id\"]\n",
    "            #print('documentID %s' % documentID)\n",
    "\n",
    "            #get the url to retrieve xml record from catalog\n",
    "            metadataURLx=catalogURL + 'rest/metadata/item/' + documentID + '/xml'\n",
    "\n",
    "            #print (\"metadata URL: \", metadataURLx)\n",
    "\n",
    "            #get the xml record\n",
    "            #the_page = requests.get(metadataURLx)\n",
    "            #print(\"metadata record %s\" % the_page.text)\n",
    "            #tree is an element tree\n",
    "            try:\n",
    "                tree = etree.parse(metadataURLx)\n",
    "                #root = etree.tostring(tree.getroot())\n",
    "                root = tree.getroot()\n",
    "            except:\n",
    "                print(\"problem parsing %s\" % metadataURLx)\n",
    "                continue\n",
    "            #print(\"xml root %s, %s\" % (root.tag, root.tag.find('http://www.isotc211.org/2005')>-1))\n",
    "            if (root.tag.find('http://www.isotc211.org/2005')==-1):\n",
    "                print(\"%s not an ISO metadata document\" % documentID)\n",
    "                continue\n",
    "\n",
    "            #docinfo = tree.docinfo\n",
    "            #print(docinfo.xml_version)\n",
    "            #print(tree.findall(\"//gmd:MD_DigitalTransferOptions\",namespaces=NSMAP))\n",
    "            for  elt in tree.getiterator(\"{http://www.isotc211.org/2005/gmd}MD_DigitalTransferOptions\"):\n",
    "                # OnlineResources that are in distribution//MD_DigitalTransferOptions\n",
    "                #print elt.text\n",
    "                count = getlinks(documentID, metadataURLx,filehandle, elt)\n",
    "                totallinkcount = totallinkcount + count\n",
    "           \n",
    "            for  elt in tree.getiterator(\"{http://www.isotc211.org/2005/srv}srv:SV_OperationMetadata\"):\n",
    "                # OnlineResources that are in SV_ServiceIdentification//SV_OperationMetadata\n",
    "                #print elt.text\n",
    "                count = getlinks(documentID, metadataURLx,filehandle, elt)\n",
    "                totallinkcount = totallinkcount + count\n",
    "             \n",
    "            counter=counter + 1\n",
    "            # in case the elastic search call gets more records than the record limit (for testing)\n",
    "            if counter > recordlimit:\n",
    "                break\n",
    "        \n",
    "        print (\"counter = %s, totallinkcount = %s, filecount = %s\" % (counter,totallinkcount,filecount))\n",
    "        filehandle.close()\n",
    "        filecount = filecount + 1\n",
    "\n",
    "\n",
    "print (\"done, counter = %s, totallinkcount = %s\" % (counter,totallinkcount))\n",
    "#print(dist_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python36-anaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
